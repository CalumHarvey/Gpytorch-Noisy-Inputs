{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import UncertainKernel\n",
    "import UncertainMeanZero\n",
    "import math\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "train_x_mean = torch.linspace(0, 1, 20)\n",
    "# We'll assume the variance shrinks the closer we get to 1\n",
    "train_x_stdv = torch.linspace(0.03, 0.01, 20)\n",
    "inputsZipped = torch.tensor(list(zip(train_x_mean, train_x_stdv)))\n",
    "\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x_mean * (2 * math.pi)) + torch.randn(train_x_mean.size()) * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2998fc8c750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "ax.errorbar(train_x_mean, train_y, xerr=(train_x_stdv * 2), fmt=\"k*\", label=\"Train Data\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = UncertainMeanZero.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(UncertainKernel.UncertainKernel())\n",
    "        # self.covar_module = LaplacianKernel.LaplacianKernel()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(inputsZipped, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/500 - Loss: 9.370   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/500 - Loss: 9.334   lengthscale: 0.693   noise: 0.667\n",
      "Iter 3/500 - Loss: 9.297   lengthscale: 0.693   noise: 0.641\n",
      "Iter 4/500 - Loss: 9.259   lengthscale: 0.693   noise: 0.616\n",
      "Iter 5/500 - Loss: 9.220   lengthscale: 0.693   noise: 0.591\n",
      "Iter 6/500 - Loss: 9.180   lengthscale: 0.693   noise: 0.566\n",
      "Iter 7/500 - Loss: 9.140   lengthscale: 0.693   noise: 0.542\n",
      "Iter 8/500 - Loss: 9.099   lengthscale: 0.693   noise: 0.518\n",
      "Iter 9/500 - Loss: 9.057   lengthscale: 0.693   noise: 0.495\n",
      "Iter 10/500 - Loss: 9.014   lengthscale: 0.693   noise: 0.472\n",
      "Iter 11/500 - Loss: 8.971   lengthscale: 0.693   noise: 0.449\n",
      "Iter 12/500 - Loss: 8.927   lengthscale: 0.693   noise: 0.427\n",
      "Iter 13/500 - Loss: 8.882   lengthscale: 0.693   noise: 0.406\n",
      "Iter 14/500 - Loss: 8.837   lengthscale: 0.693   noise: 0.385\n",
      "Iter 15/500 - Loss: 8.791   lengthscale: 0.693   noise: 0.365\n",
      "Iter 16/500 - Loss: 8.745   lengthscale: 0.693   noise: 0.345\n",
      "Iter 17/500 - Loss: 8.698   lengthscale: 0.693   noise: 0.325\n",
      "Iter 18/500 - Loss: 8.651   lengthscale: 0.693   noise: 0.307\n",
      "Iter 19/500 - Loss: 8.603   lengthscale: 0.693   noise: 0.288\n",
      "Iter 20/500 - Loss: 8.555   lengthscale: 0.693   noise: 0.271\n",
      "Iter 21/500 - Loss: 8.506   lengthscale: 0.693   noise: 0.254\n",
      "Iter 22/500 - Loss: 8.457   lengthscale: 0.693   noise: 0.238\n",
      "Iter 23/500 - Loss: 8.407   lengthscale: 0.693   noise: 0.222\n",
      "Iter 24/500 - Loss: 8.358   lengthscale: 0.693   noise: 0.207\n",
      "Iter 25/500 - Loss: 8.308   lengthscale: 0.693   noise: 0.193\n",
      "Iter 26/500 - Loss: 8.257   lengthscale: 0.693   noise: 0.179\n",
      "Iter 27/500 - Loss: 8.206   lengthscale: 0.693   noise: 0.166\n",
      "Iter 28/500 - Loss: 8.155   lengthscale: 0.693   noise: 0.154\n",
      "Iter 29/500 - Loss: 8.104   lengthscale: 0.693   noise: 0.142\n",
      "Iter 30/500 - Loss: 8.053   lengthscale: 0.693   noise: 0.131\n",
      "Iter 31/500 - Loss: 8.001   lengthscale: 0.693   noise: 0.121\n",
      "Iter 32/500 - Loss: 7.949   lengthscale: 0.693   noise: 0.111\n",
      "Iter 33/500 - Loss: 7.898   lengthscale: 0.693   noise: 0.102\n",
      "Iter 34/500 - Loss: 7.845   lengthscale: 0.693   noise: 0.094\n",
      "Iter 35/500 - Loss: 7.793   lengthscale: 0.693   noise: 0.086\n",
      "Iter 36/500 - Loss: 7.741   lengthscale: 0.693   noise: 0.079\n",
      "Iter 37/500 - Loss: 7.689   lengthscale: 0.693   noise: 0.072\n",
      "Iter 38/500 - Loss: 7.636   lengthscale: 0.693   noise: 0.066\n",
      "Iter 39/500 - Loss: 7.584   lengthscale: 0.693   noise: 0.060\n",
      "Iter 40/500 - Loss: 7.531   lengthscale: 0.693   noise: 0.055\n",
      "Iter 41/500 - Loss: 7.478   lengthscale: 0.693   noise: 0.050\n",
      "Iter 42/500 - Loss: 7.426   lengthscale: 0.693   noise: 0.045\n",
      "Iter 43/500 - Loss: 7.373   lengthscale: 0.693   noise: 0.041\n",
      "Iter 44/500 - Loss: 7.320   lengthscale: 0.693   noise: 0.037\n",
      "Iter 45/500 - Loss: 7.267   lengthscale: 0.693   noise: 0.034\n",
      "Iter 46/500 - Loss: 7.214   lengthscale: 0.693   noise: 0.031\n",
      "Iter 47/500 - Loss: 7.161   lengthscale: 0.693   noise: 0.028\n",
      "Iter 48/500 - Loss: 7.109   lengthscale: 0.693   noise: 0.025\n",
      "Iter 49/500 - Loss: 7.056   lengthscale: 0.693   noise: 0.023\n",
      "Iter 50/500 - Loss: 7.003   lengthscale: 0.693   noise: 0.020\n",
      "Iter 51/500 - Loss: 6.950   lengthscale: 0.693   noise: 0.018\n",
      "Iter 52/500 - Loss: 6.897   lengthscale: 0.693   noise: 0.017\n",
      "Iter 53/500 - Loss: 6.844   lengthscale: 0.693   noise: 0.015\n",
      "Iter 54/500 - Loss: 6.791   lengthscale: 0.693   noise: 0.014\n",
      "Iter 55/500 - Loss: 6.739   lengthscale: 0.693   noise: 0.012\n",
      "Iter 56/500 - Loss: 6.686   lengthscale: 0.693   noise: 0.011\n",
      "Iter 57/500 - Loss: 6.633   lengthscale: 0.693   noise: 0.010\n",
      "Iter 58/500 - Loss: 6.580   lengthscale: 0.693   noise: 0.009\n",
      "Iter 59/500 - Loss: 6.527   lengthscale: 0.693   noise: 0.008\n",
      "Iter 60/500 - Loss: 6.475   lengthscale: 0.693   noise: 0.007\n",
      "Iter 61/500 - Loss: 6.422   lengthscale: 0.693   noise: 0.007\n",
      "Iter 62/500 - Loss: 6.369   lengthscale: 0.693   noise: 0.006\n",
      "Iter 63/500 - Loss: 6.317   lengthscale: 0.693   noise: 0.005\n",
      "Iter 64/500 - Loss: 6.264   lengthscale: 0.693   noise: 0.005\n",
      "Iter 65/500 - Loss: 6.211   lengthscale: 0.693   noise: 0.004\n",
      "Iter 66/500 - Loss: 6.159   lengthscale: 0.693   noise: 0.004\n",
      "Iter 67/500 - Loss: 6.106   lengthscale: 0.693   noise: 0.004\n",
      "Iter 68/500 - Loss: 6.054   lengthscale: 0.693   noise: 0.003\n",
      "Iter 69/500 - Loss: 6.001   lengthscale: 0.693   noise: 0.003\n",
      "Iter 70/500 - Loss: 5.949   lengthscale: 0.694   noise: 0.003\n",
      "Iter 71/500 - Loss: 5.896   lengthscale: 0.694   noise: 0.002\n",
      "Iter 72/500 - Loss: 5.844   lengthscale: 0.694   noise: 0.002\n",
      "Iter 73/500 - Loss: 5.791   lengthscale: 0.694   noise: 0.002\n",
      "Iter 74/500 - Loss: 5.739   lengthscale: 0.694   noise: 0.002\n",
      "Iter 75/500 - Loss: 5.687   lengthscale: 0.694   noise: 0.002\n",
      "Iter 76/500 - Loss: 5.634   lengthscale: 0.694   noise: 0.002\n",
      "Iter 77/500 - Loss: 5.582   lengthscale: 0.694   noise: 0.001\n",
      "Iter 78/500 - Loss: 5.530   lengthscale: 0.694   noise: 0.001\n",
      "Iter 79/500 - Loss: 5.478   lengthscale: 0.694   noise: 0.001\n",
      "Iter 80/500 - Loss: 5.425   lengthscale: 0.694   noise: 0.001\n",
      "Iter 81/500 - Loss: 5.373   lengthscale: 0.694   noise: 0.001\n",
      "Iter 82/500 - Loss: 5.321   lengthscale: 0.694   noise: 0.001\n",
      "Iter 83/500 - Loss: 5.269   lengthscale: 0.694   noise: 0.001\n",
      "Iter 84/500 - Loss: 5.217   lengthscale: 0.694   noise: 0.001\n",
      "Iter 85/500 - Loss: 5.165   lengthscale: 0.694   noise: 0.001\n",
      "Iter 86/500 - Loss: 5.113   lengthscale: 0.694   noise: 0.001\n",
      "Iter 87/500 - Loss: 5.061   lengthscale: 0.694   noise: 0.001\n",
      "Iter 88/500 - Loss: 5.009   lengthscale: 0.695   noise: 0.001\n",
      "Iter 89/500 - Loss: 4.957   lengthscale: 0.695   noise: 0.000\n",
      "Iter 90/500 - Loss: 4.905   lengthscale: 0.695   noise: 0.000\n",
      "Iter 91/500 - Loss: 4.853   lengthscale: 0.695   noise: 0.000\n",
      "Iter 92/500 - Loss: 4.801   lengthscale: 0.695   noise: 0.000\n",
      "Iter 93/500 - Loss: 4.749   lengthscale: 0.695   noise: 0.000\n",
      "Iter 94/500 - Loss: 4.697   lengthscale: 0.695   noise: 0.000\n",
      "Iter 95/500 - Loss: 4.646   lengthscale: 0.695   noise: 0.000\n",
      "Iter 96/500 - Loss: 4.594   lengthscale: 0.696   noise: 0.000\n",
      "Iter 97/500 - Loss: 4.542   lengthscale: 0.696   noise: 0.000\n",
      "Iter 98/500 - Loss: 4.490   lengthscale: 0.696   noise: 0.000\n",
      "Iter 99/500 - Loss: 4.438   lengthscale: 0.696   noise: 0.000\n",
      "Iter 100/500 - Loss: 4.387   lengthscale: 0.697   noise: 0.000\n",
      "Iter 101/500 - Loss: 4.335   lengthscale: 0.697   noise: 0.000\n",
      "Iter 102/500 - Loss: 4.283   lengthscale: 0.697   noise: 0.000\n",
      "Iter 103/500 - Loss: 4.232   lengthscale: 0.697   noise: 0.000\n",
      "Iter 104/500 - Loss: 4.180   lengthscale: 0.697   noise: 0.000\n",
      "Iter 105/500 - Loss: 4.128   lengthscale: 0.697   noise: 0.000\n",
      "Iter 106/500 - Loss: 4.077   lengthscale: 0.698   noise: 0.000\n",
      "Iter 107/500 - Loss: 4.025   lengthscale: 0.698   noise: 0.000\n",
      "Iter 108/500 - Loss: 3.974   lengthscale: 0.698   noise: 0.000\n",
      "Iter 109/500 - Loss: 3.922   lengthscale: 0.699   noise: 0.000\n",
      "Iter 110/500 - Loss: 3.871   lengthscale: 0.699   noise: 0.000\n",
      "Iter 111/500 - Loss: 3.819   lengthscale: 0.700   noise: 0.000\n",
      "Iter 112/500 - Loss: 3.768   lengthscale: 0.701   noise: 0.000\n",
      "Iter 113/500 - Loss: 3.717   lengthscale: 0.702   noise: 0.000\n",
      "Iter 114/500 - Loss: 3.665   lengthscale: 0.702   noise: 0.000\n",
      "Iter 115/500 - Loss: 3.614   lengthscale: 0.703   noise: 0.000\n",
      "Iter 116/500 - Loss: 3.563   lengthscale: 0.704   noise: 0.000\n",
      "Iter 117/500 - Loss: 3.511   lengthscale: 0.704   noise: 0.000\n",
      "Iter 118/500 - Loss: 3.460   lengthscale: 0.706   noise: 0.000\n",
      "Iter 119/500 - Loss: 3.409   lengthscale: 0.707   noise: 0.000\n",
      "Iter 120/500 - Loss: 3.358   lengthscale: 0.709   noise: 0.000\n",
      "Iter 121/500 - Loss: 3.307   lengthscale: 0.711   noise: 0.000\n",
      "Iter 122/500 - Loss: 3.256   lengthscale: 0.713   noise: 0.000\n",
      "Iter 123/500 - Loss: 3.205   lengthscale: 0.716   noise: 0.000\n",
      "Iter 124/500 - Loss: 3.154   lengthscale: 0.719   noise: 0.000\n",
      "Iter 125/500 - Loss: 3.103   lengthscale: 0.722   noise: 0.000\n",
      "Iter 126/500 - Loss: 3.052   lengthscale: 0.726   noise: 0.000\n",
      "Iter 127/500 - Loss: 3.001   lengthscale: 0.730   noise: 0.000\n",
      "Iter 128/500 - Loss: 2.950   lengthscale: 0.734   noise: 0.000\n",
      "Iter 129/500 - Loss: 2.900   lengthscale: 0.739   noise: 0.000\n",
      "Iter 130/500 - Loss: 2.849   lengthscale: 0.744   noise: 0.000\n",
      "Iter 131/500 - Loss: 2.799   lengthscale: 0.750   noise: 0.000\n",
      "Iter 132/500 - Loss: 2.749   lengthscale: 0.757   noise: 0.000\n",
      "Iter 133/500 - Loss: 2.699   lengthscale: 0.764   noise: 0.000\n",
      "Iter 134/500 - Loss: 2.648   lengthscale: 0.773   noise: 0.000\n",
      "Iter 135/500 - Loss: 2.599   lengthscale: 0.782   noise: 0.000\n",
      "Iter 136/500 - Loss: 2.549   lengthscale: 0.792   noise: 0.000\n",
      "Iter 137/500 - Loss: 2.499   lengthscale: 0.803   noise: 0.000\n",
      "Iter 138/500 - Loss: 2.450   lengthscale: 0.815   noise: 0.000\n",
      "Iter 139/500 - Loss: 2.401   lengthscale: 0.828   noise: 0.000\n",
      "Iter 140/500 - Loss: 2.352   lengthscale: 0.843   noise: 0.000\n",
      "Iter 141/500 - Loss: 2.303   lengthscale: 0.861   noise: 0.000\n",
      "Iter 142/500 - Loss: 2.255   lengthscale: 0.880   noise: 0.000\n",
      "Iter 143/500 - Loss: 2.206   lengthscale: 0.902   noise: 0.000\n",
      "Iter 144/500 - Loss: 2.159   lengthscale: 0.927   noise: 0.000\n",
      "Iter 145/500 - Loss: 2.111   lengthscale: 0.955   noise: 0.000\n",
      "Iter 146/500 - Loss: 2.064   lengthscale: 0.986   noise: 0.000\n",
      "Iter 147/500 - Loss: 2.017   lengthscale: 1.020   noise: 0.000\n",
      "Iter 148/500 - Loss: 1.971   lengthscale: 1.058   noise: 0.000\n",
      "Iter 149/500 - Loss: 1.926   lengthscale: 1.101   noise: 0.000\n",
      "Iter 150/500 - Loss: 1.880   lengthscale: 1.148   noise: 0.000\n",
      "Iter 151/500 - Loss: 1.836   lengthscale: 1.200   noise: 0.000\n",
      "Iter 152/500 - Loss: 1.792   lengthscale: 1.257   noise: 0.000\n",
      "Iter 153/500 - Loss: 1.749   lengthscale: 1.320   noise: 0.000\n",
      "Iter 154/500 - Loss: 1.707   lengthscale: 1.389   noise: 0.000\n",
      "Iter 155/500 - Loss: 1.665   lengthscale: 1.464   noise: 0.000\n",
      "Iter 156/500 - Loss: 1.625   lengthscale: 1.546   noise: 0.000\n",
      "Iter 157/500 - Loss: 1.586   lengthscale: 1.636   noise: 0.000\n",
      "Iter 158/500 - Loss: 1.547   lengthscale: 1.733   noise: 0.000\n",
      "Iter 159/500 - Loss: 1.510   lengthscale: 1.838   noise: 0.000\n",
      "Iter 160/500 - Loss: 1.474   lengthscale: 1.950   noise: 0.000\n",
      "Iter 161/500 - Loss: 1.440   lengthscale: 2.071   noise: 0.000\n",
      "Iter 162/500 - Loss: 1.407   lengthscale: 2.201   noise: 0.000\n",
      "Iter 163/500 - Loss: 1.375   lengthscale: 2.339   noise: 0.000\n",
      "Iter 164/500 - Loss: 1.345   lengthscale: 2.485   noise: 0.000\n",
      "Iter 165/500 - Loss: 1.317   lengthscale: 2.639   noise: 0.000\n",
      "Iter 166/500 - Loss: 1.291   lengthscale: 2.800   noise: 0.000\n",
      "Iter 167/500 - Loss: 1.266   lengthscale: 2.969   noise: 0.000\n",
      "Iter 168/500 - Loss: 1.244   lengthscale: 3.145   noise: 0.000\n",
      "Iter 169/500 - Loss: 1.223   lengthscale: 3.327   noise: 0.000\n",
      "Iter 170/500 - Loss: 1.205   lengthscale: 3.516   noise: 0.000\n",
      "Iter 171/500 - Loss: 1.188   lengthscale: 3.710   noise: 0.000\n",
      "Iter 172/500 - Loss: 1.174   lengthscale: 3.909   noise: 0.000\n",
      "Iter 173/500 - Loss: 1.161   lengthscale: 4.113   noise: 0.000\n",
      "Iter 174/500 - Loss: 1.151   lengthscale: 4.321   noise: 0.000\n",
      "Iter 175/500 - Loss: 1.142   lengthscale: 4.534   noise: 0.000\n",
      "Iter 176/500 - Loss: 1.135   lengthscale: 4.749   noise: 0.000\n",
      "Iter 177/500 - Loss: 1.130   lengthscale: 4.968   noise: 0.000\n",
      "Iter 178/500 - Loss: 1.126   lengthscale: 5.190   noise: 0.000\n",
      "Iter 179/500 - Loss: 1.124   lengthscale: 5.414   noise: 0.000\n",
      "Iter 180/500 - Loss: 1.122   lengthscale: 5.640   noise: 0.000\n",
      "Iter 181/500 - Loss: 1.122   lengthscale: 5.868   noise: 0.000\n",
      "Iter 182/500 - Loss: 1.122   lengthscale: 6.097   noise: 0.000\n",
      "Iter 183/500 - Loss: 1.123   lengthscale: 6.327   noise: 0.000\n",
      "Iter 184/500 - Loss: 1.124   lengthscale: 6.558   noise: 0.000\n",
      "Iter 185/500 - Loss: 1.126   lengthscale: 6.790   noise: 0.000\n",
      "Iter 186/500 - Loss: 1.127   lengthscale: 7.021   noise: 0.000\n",
      "Iter 187/500 - Loss: 1.129   lengthscale: 7.253   noise: 0.000\n",
      "Iter 188/500 - Loss: 1.130   lengthscale: 7.484   noise: 0.000\n",
      "Iter 189/500 - Loss: 1.131   lengthscale: 7.714   noise: 0.000\n",
      "Iter 190/500 - Loss: 1.131   lengthscale: 7.943   noise: 0.000\n",
      "Iter 191/500 - Loss: 1.132   lengthscale: 8.170   noise: 0.000\n",
      "Iter 192/500 - Loss: 1.132   lengthscale: 8.396   noise: 0.000\n",
      "Iter 193/500 - Loss: 1.132   lengthscale: 8.620   noise: 0.000\n",
      "Iter 194/500 - Loss: 1.131   lengthscale: 8.842   noise: 0.000\n",
      "Iter 195/500 - Loss: 1.131   lengthscale: 9.062   noise: 0.000\n",
      "Iter 196/500 - Loss: 1.130   lengthscale: 9.279   noise: 0.000\n",
      "Iter 197/500 - Loss: 1.129   lengthscale: 9.493   noise: 0.000\n",
      "Iter 198/500 - Loss: 1.128   lengthscale: 9.704   noise: 0.000\n",
      "Iter 199/500 - Loss: 1.127   lengthscale: 9.912   noise: 0.000\n",
      "Iter 200/500 - Loss: 1.127   lengthscale: 10.117   noise: 0.000\n",
      "Iter 201/500 - Loss: 1.126   lengthscale: 10.319   noise: 0.000\n",
      "Iter 202/500 - Loss: 1.125   lengthscale: 10.517   noise: 0.000\n",
      "Iter 203/500 - Loss: 1.124   lengthscale: 10.712   noise: 0.000\n",
      "Iter 204/500 - Loss: 1.124   lengthscale: 10.904   noise: 0.000\n",
      "Iter 205/500 - Loss: 1.123   lengthscale: 11.092   noise: 0.000\n",
      "Iter 206/500 - Loss: 1.123   lengthscale: 11.276   noise: 0.000\n",
      "Iter 207/500 - Loss: 1.122   lengthscale: 11.458   noise: 0.000\n",
      "Iter 208/500 - Loss: 1.122   lengthscale: 11.635   noise: 0.000\n",
      "Iter 209/500 - Loss: 1.122   lengthscale: 11.810   noise: 0.000\n",
      "Iter 210/500 - Loss: 1.122   lengthscale: 11.981   noise: 0.000\n",
      "Iter 211/500 - Loss: 1.122   lengthscale: 12.149   noise: 0.000\n",
      "Iter 212/500 - Loss: 1.122   lengthscale: 12.314   noise: 0.000\n",
      "Iter 213/500 - Loss: 1.122   lengthscale: 12.476   noise: 0.000\n",
      "Iter 214/500 - Loss: 1.122   lengthscale: 12.635   noise: 0.000\n",
      "Iter 215/500 - Loss: 1.122   lengthscale: 12.791   noise: 0.000\n",
      "Iter 216/500 - Loss: 1.122   lengthscale: 12.944   noise: 0.000\n",
      "Iter 217/500 - Loss: 1.122   lengthscale: 13.095   noise: 0.000\n",
      "Iter 218/500 - Loss: 1.122   lengthscale: 13.244   noise: 0.000\n",
      "Iter 219/500 - Loss: 1.122   lengthscale: 13.390   noise: 0.000\n",
      "Iter 220/500 - Loss: 1.122   lengthscale: 13.534   noise: 0.000\n",
      "Iter 221/500 - Loss: 1.122   lengthscale: 13.675   noise: 0.000\n",
      "Iter 222/500 - Loss: 1.122   lengthscale: 13.815   noise: 0.000\n",
      "Iter 223/500 - Loss: 1.122   lengthscale: 13.953   noise: 0.000\n",
      "Iter 224/500 - Loss: 1.122   lengthscale: 14.089   noise: 0.000\n",
      "Iter 225/500 - Loss: 1.122   lengthscale: 14.224   noise: 0.000\n",
      "Iter 226/500 - Loss: 1.122   lengthscale: 14.357   noise: 0.000\n",
      "Iter 227/500 - Loss: 1.122   lengthscale: 14.488   noise: 0.000\n",
      "Iter 228/500 - Loss: 1.122   lengthscale: 14.619   noise: 0.000\n",
      "Iter 229/500 - Loss: 1.122   lengthscale: 14.747   noise: 0.000\n",
      "Iter 230/500 - Loss: 1.122   lengthscale: 14.875   noise: 0.000\n",
      "Iter 231/500 - Loss: 1.122   lengthscale: 15.002   noise: 0.000\n",
      "Iter 232/500 - Loss: 1.122   lengthscale: 15.127   noise: 0.000\n",
      "Iter 233/500 - Loss: 1.122   lengthscale: 15.252   noise: 0.000\n",
      "Iter 234/500 - Loss: 1.122   lengthscale: 15.376   noise: 0.000\n",
      "Iter 235/500 - Loss: 1.122   lengthscale: 15.499   noise: 0.000\n",
      "Iter 236/500 - Loss: 1.122   lengthscale: 15.621   noise: 0.000\n",
      "Iter 237/500 - Loss: 1.122   lengthscale: 15.742   noise: 0.000\n",
      "Iter 238/500 - Loss: 1.122   lengthscale: 15.863   noise: 0.000\n",
      "Iter 239/500 - Loss: 1.122   lengthscale: 15.983   noise: 0.000\n",
      "Iter 240/500 - Loss: 1.122   lengthscale: 16.102   noise: 0.000\n",
      "Iter 241/500 - Loss: 1.122   lengthscale: 16.221   noise: 0.000\n",
      "Iter 242/500 - Loss: 1.122   lengthscale: 16.340   noise: 0.000\n",
      "Iter 243/500 - Loss: 1.122   lengthscale: 16.457   noise: 0.000\n",
      "Iter 244/500 - Loss: 1.122   lengthscale: 16.575   noise: 0.000\n",
      "Iter 245/500 - Loss: 1.122   lengthscale: 16.691   noise: 0.000\n",
      "Iter 246/500 - Loss: 1.122   lengthscale: 16.808   noise: 0.000\n",
      "Iter 247/500 - Loss: 1.122   lengthscale: 16.923   noise: 0.000\n",
      "Iter 248/500 - Loss: 1.122   lengthscale: 17.039   noise: 0.000\n",
      "Iter 249/500 - Loss: 1.122   lengthscale: 17.154   noise: 0.000\n",
      "Iter 250/500 - Loss: 1.122   lengthscale: 17.268   noise: 0.000\n",
      "Iter 251/500 - Loss: 1.122   lengthscale: 17.382   noise: 0.000\n",
      "Iter 252/500 - Loss: 1.122   lengthscale: 17.496   noise: 0.000\n",
      "Iter 253/500 - Loss: 1.122   lengthscale: 17.609   noise: 0.000\n",
      "Iter 254/500 - Loss: 1.122   lengthscale: 17.721   noise: 0.000\n",
      "Iter 255/500 - Loss: 1.122   lengthscale: 17.834   noise: 0.000\n",
      "Iter 256/500 - Loss: 1.122   lengthscale: 17.946   noise: 0.000\n",
      "Iter 257/500 - Loss: 1.122   lengthscale: 18.057   noise: 0.000\n",
      "Iter 258/500 - Loss: 1.122   lengthscale: 18.168   noise: 0.000\n",
      "Iter 259/500 - Loss: 1.122   lengthscale: 18.278   noise: 0.000\n",
      "Iter 260/500 - Loss: 1.122   lengthscale: 18.389   noise: 0.000\n",
      "Iter 261/500 - Loss: 1.122   lengthscale: 18.498   noise: 0.000\n",
      "Iter 262/500 - Loss: 1.122   lengthscale: 18.607   noise: 0.000\n",
      "Iter 263/500 - Loss: 1.122   lengthscale: 18.716   noise: 0.000\n",
      "Iter 264/500 - Loss: 1.122   lengthscale: 18.825   noise: 0.000\n",
      "Iter 265/500 - Loss: 1.122   lengthscale: 18.933   noise: 0.000\n",
      "Iter 266/500 - Loss: 1.122   lengthscale: 19.040   noise: 0.000\n",
      "Iter 267/500 - Loss: 1.122   lengthscale: 19.147   noise: 0.000\n",
      "Iter 268/500 - Loss: 1.122   lengthscale: 19.254   noise: 0.000\n",
      "Iter 269/500 - Loss: 1.122   lengthscale: 19.360   noise: 0.000\n",
      "Iter 270/500 - Loss: 1.122   lengthscale: 19.466   noise: 0.000\n",
      "Iter 271/500 - Loss: 1.122   lengthscale: 19.571   noise: 0.000\n",
      "Iter 272/500 - Loss: 1.122   lengthscale: 19.677   noise: 0.000\n",
      "Iter 273/500 - Loss: 1.122   lengthscale: 19.781   noise: 0.000\n",
      "Iter 274/500 - Loss: 1.122   lengthscale: 19.885   noise: 0.000\n",
      "Iter 275/500 - Loss: 1.122   lengthscale: 19.989   noise: 0.000\n",
      "Iter 276/500 - Loss: 1.122   lengthscale: 20.093   noise: 0.000\n",
      "Iter 277/500 - Loss: 1.122   lengthscale: 20.196   noise: 0.000\n",
      "Iter 278/500 - Loss: 1.122   lengthscale: 20.299   noise: 0.000\n",
      "Iter 279/500 - Loss: 1.122   lengthscale: 20.401   noise: 0.000\n",
      "Iter 280/500 - Loss: 1.122   lengthscale: 20.503   noise: 0.000\n",
      "Iter 281/500 - Loss: 1.122   lengthscale: 20.605   noise: 0.000\n",
      "Iter 282/500 - Loss: 1.122   lengthscale: 20.706   noise: 0.000\n",
      "Iter 283/500 - Loss: 1.122   lengthscale: 20.807   noise: 0.000\n",
      "Iter 284/500 - Loss: 1.122   lengthscale: 20.908   noise: 0.000\n",
      "Iter 285/500 - Loss: 1.122   lengthscale: 21.008   noise: 0.000\n",
      "Iter 286/500 - Loss: 1.122   lengthscale: 21.108   noise: 0.000\n",
      "Iter 287/500 - Loss: 1.122   lengthscale: 21.208   noise: 0.000\n",
      "Iter 288/500 - Loss: 1.122   lengthscale: 21.308   noise: 0.000\n",
      "Iter 289/500 - Loss: 1.122   lengthscale: 21.407   noise: 0.000\n",
      "Iter 290/500 - Loss: 1.122   lengthscale: 21.506   noise: 0.000\n",
      "Iter 291/500 - Loss: 1.122   lengthscale: 21.604   noise: 0.000\n",
      "Iter 292/500 - Loss: 1.122   lengthscale: 21.703   noise: 0.000\n",
      "Iter 293/500 - Loss: 1.122   lengthscale: 21.801   noise: 0.000\n",
      "Iter 294/500 - Loss: 1.122   lengthscale: 21.899   noise: 0.000\n",
      "Iter 295/500 - Loss: 1.122   lengthscale: 21.996   noise: 0.000\n",
      "Iter 296/500 - Loss: 1.122   lengthscale: 22.093   noise: 0.000\n",
      "Iter 297/500 - Loss: 1.122   lengthscale: 22.190   noise: 0.000\n",
      "Iter 298/500 - Loss: 1.122   lengthscale: 22.287   noise: 0.000\n",
      "Iter 299/500 - Loss: 1.122   lengthscale: 22.384   noise: 0.000\n",
      "Iter 300/500 - Loss: 1.122   lengthscale: 22.480   noise: 0.000\n",
      "Iter 301/500 - Loss: 1.122   lengthscale: 22.576   noise: 0.000\n",
      "Iter 302/500 - Loss: 1.122   lengthscale: 22.672   noise: 0.000\n",
      "Iter 303/500 - Loss: 1.122   lengthscale: 22.768   noise: 0.000\n",
      "Iter 304/500 - Loss: 1.122   lengthscale: 22.863   noise: 0.000\n",
      "Iter 305/500 - Loss: 1.122   lengthscale: 22.958   noise: 0.000\n",
      "Iter 306/500 - Loss: 1.122   lengthscale: 23.053   noise: 0.000\n",
      "Iter 307/500 - Loss: 1.122   lengthscale: 23.148   noise: 0.000\n",
      "Iter 308/500 - Loss: 1.122   lengthscale: 23.242   noise: 0.000\n",
      "Iter 309/500 - Loss: 1.122   lengthscale: 23.337   noise: 0.000\n",
      "Iter 310/500 - Loss: 1.122   lengthscale: 23.431   noise: 0.000\n",
      "Iter 311/500 - Loss: 1.122   lengthscale: 23.525   noise: 0.000\n",
      "Iter 312/500 - Loss: 1.122   lengthscale: 23.618   noise: 0.000\n",
      "Iter 313/500 - Loss: 1.122   lengthscale: 23.712   noise: 0.000\n",
      "Iter 314/500 - Loss: 1.122   lengthscale: 23.805   noise: 0.000\n",
      "Iter 315/500 - Loss: 1.122   lengthscale: 23.898   noise: 0.000\n",
      "Iter 316/500 - Loss: 1.122   lengthscale: 23.991   noise: 0.000\n",
      "Iter 317/500 - Loss: 1.122   lengthscale: 24.084   noise: 0.000\n",
      "Iter 318/500 - Loss: 1.122   lengthscale: 24.176   noise: 0.000\n",
      "Iter 319/500 - Loss: 1.122   lengthscale: 24.268   noise: 0.000\n",
      "Iter 320/500 - Loss: 1.122   lengthscale: 24.361   noise: 0.000\n",
      "Iter 321/500 - Loss: 1.122   lengthscale: 24.452   noise: 0.000\n",
      "Iter 322/500 - Loss: 1.122   lengthscale: 24.544   noise: 0.000\n",
      "Iter 323/500 - Loss: 1.122   lengthscale: 24.636   noise: 0.000\n",
      "Iter 324/500 - Loss: 1.122   lengthscale: 24.727   noise: 0.000\n",
      "Iter 325/500 - Loss: 1.122   lengthscale: 24.818   noise: 0.000\n",
      "Iter 326/500 - Loss: 1.122   lengthscale: 24.909   noise: 0.000\n",
      "Iter 327/500 - Loss: 1.122   lengthscale: 25.000   noise: 0.000\n",
      "Iter 328/500 - Loss: 1.122   lengthscale: 25.091   noise: 0.000\n",
      "Iter 329/500 - Loss: 1.122   lengthscale: 25.181   noise: 0.000\n",
      "Iter 330/500 - Loss: 1.122   lengthscale: 25.271   noise: 0.000\n",
      "Iter 331/500 - Loss: 1.122   lengthscale: 25.362   noise: 0.000\n",
      "Iter 332/500 - Loss: 1.122   lengthscale: 25.452   noise: 0.000\n",
      "Iter 333/500 - Loss: 1.122   lengthscale: 25.541   noise: 0.000\n",
      "Iter 334/500 - Loss: 1.122   lengthscale: 25.631   noise: 0.000\n",
      "Iter 335/500 - Loss: 1.122   lengthscale: 25.720   noise: 0.000\n",
      "Iter 336/500 - Loss: 1.122   lengthscale: 25.810   noise: 0.000\n",
      "Iter 337/500 - Loss: 1.122   lengthscale: 25.899   noise: 0.000\n",
      "Iter 338/500 - Loss: 1.122   lengthscale: 25.988   noise: 0.000\n",
      "Iter 339/500 - Loss: 1.122   lengthscale: 26.077   noise: 0.000\n",
      "Iter 340/500 - Loss: 1.122   lengthscale: 26.165   noise: 0.000\n",
      "Iter 341/500 - Loss: 1.122   lengthscale: 26.254   noise: 0.000\n",
      "Iter 342/500 - Loss: 1.122   lengthscale: 26.342   noise: 0.000\n",
      "Iter 343/500 - Loss: 1.122   lengthscale: 26.431   noise: 0.000\n",
      "Iter 344/500 - Loss: 1.122   lengthscale: 26.519   noise: 0.000\n",
      "Iter 345/500 - Loss: 1.122   lengthscale: 26.607   noise: 0.000\n",
      "Iter 346/500 - Loss: 1.122   lengthscale: 26.694   noise: 0.000\n",
      "Iter 347/500 - Loss: 1.122   lengthscale: 26.782   noise: 0.000\n",
      "Iter 348/500 - Loss: 1.122   lengthscale: 26.870   noise: 0.000\n",
      "Iter 349/500 - Loss: 1.122   lengthscale: 26.957   noise: 0.000\n",
      "Iter 350/500 - Loss: 1.122   lengthscale: 27.044   noise: 0.000\n",
      "Iter 351/500 - Loss: 1.122   lengthscale: 27.131   noise: 0.000\n",
      "Iter 352/500 - Loss: 1.122   lengthscale: 27.218   noise: 0.000\n",
      "Iter 353/500 - Loss: 1.122   lengthscale: 27.305   noise: 0.000\n",
      "Iter 354/500 - Loss: 1.122   lengthscale: 27.392   noise: 0.000\n",
      "Iter 355/500 - Loss: 1.122   lengthscale: 27.478   noise: 0.000\n",
      "Iter 356/500 - Loss: 1.122   lengthscale: 27.565   noise: 0.000\n",
      "Iter 357/500 - Loss: 1.122   lengthscale: 27.651   noise: 0.000\n",
      "Iter 358/500 - Loss: 1.122   lengthscale: 27.737   noise: 0.000\n",
      "Iter 359/500 - Loss: 1.122   lengthscale: 27.823   noise: 0.000\n",
      "Iter 360/500 - Loss: 1.122   lengthscale: 27.909   noise: 0.000\n",
      "Iter 361/500 - Loss: 1.122   lengthscale: 27.995   noise: 0.000\n",
      "Iter 362/500 - Loss: 1.122   lengthscale: 28.080   noise: 0.000\n",
      "Iter 363/500 - Loss: 1.122   lengthscale: 28.166   noise: 0.000\n",
      "Iter 364/500 - Loss: 1.122   lengthscale: 28.251   noise: 0.000\n",
      "Iter 365/500 - Loss: 1.122   lengthscale: 28.337   noise: 0.000\n",
      "Iter 366/500 - Loss: 1.122   lengthscale: 28.422   noise: 0.000\n",
      "Iter 367/500 - Loss: 1.122   lengthscale: 28.507   noise: 0.000\n",
      "Iter 368/500 - Loss: 1.122   lengthscale: 28.592   noise: 0.000\n",
      "Iter 369/500 - Loss: 1.122   lengthscale: 28.677   noise: 0.000\n",
      "Iter 370/500 - Loss: 1.122   lengthscale: 28.761   noise: 0.000\n",
      "Iter 371/500 - Loss: 1.122   lengthscale: 28.846   noise: 0.000\n",
      "Iter 372/500 - Loss: 1.122   lengthscale: 28.930   noise: 0.000\n",
      "Iter 373/500 - Loss: 1.122   lengthscale: 29.015   noise: 0.000\n",
      "Iter 374/500 - Loss: 1.122   lengthscale: 29.099   noise: 0.000\n",
      "Iter 375/500 - Loss: 1.122   lengthscale: 29.183   noise: 0.000\n",
      "Iter 376/500 - Loss: 1.122   lengthscale: 29.267   noise: 0.000\n",
      "Iter 377/500 - Loss: 1.122   lengthscale: 29.351   noise: 0.000\n",
      "Iter 378/500 - Loss: 1.122   lengthscale: 29.435   noise: 0.000\n",
      "Iter 379/500 - Loss: 1.122   lengthscale: 29.519   noise: 0.000\n",
      "Iter 380/500 - Loss: 1.122   lengthscale: 29.602   noise: 0.000\n",
      "Iter 381/500 - Loss: 1.122   lengthscale: 29.686   noise: 0.000\n",
      "Iter 382/500 - Loss: 1.122   lengthscale: 29.769   noise: 0.000\n",
      "Iter 383/500 - Loss: 1.122   lengthscale: 29.852   noise: 0.000\n",
      "Iter 384/500 - Loss: 1.122   lengthscale: 29.935   noise: 0.000\n",
      "Iter 385/500 - Loss: 1.122   lengthscale: 30.019   noise: 0.000\n",
      "Iter 386/500 - Loss: 1.122   lengthscale: 30.101   noise: 0.000\n",
      "Iter 387/500 - Loss: 1.122   lengthscale: 30.184   noise: 0.000\n",
      "Iter 388/500 - Loss: 1.122   lengthscale: 30.267   noise: 0.000\n",
      "Iter 389/500 - Loss: 1.122   lengthscale: 30.350   noise: 0.000\n",
      "Iter 390/500 - Loss: 1.122   lengthscale: 30.432   noise: 0.000\n",
      "Iter 391/500 - Loss: 1.122   lengthscale: 30.515   noise: 0.000\n",
      "Iter 392/500 - Loss: 1.122   lengthscale: 30.597   noise: 0.000\n",
      "Iter 393/500 - Loss: 1.122   lengthscale: 30.679   noise: 0.000\n",
      "Iter 394/500 - Loss: 1.122   lengthscale: 30.762   noise: 0.000\n",
      "Iter 395/500 - Loss: 1.122   lengthscale: 30.844   noise: 0.000\n",
      "Iter 396/500 - Loss: 1.122   lengthscale: 30.926   noise: 0.000\n",
      "Iter 397/500 - Loss: 1.122   lengthscale: 31.008   noise: 0.000\n",
      "Iter 398/500 - Loss: 1.122   lengthscale: 31.089   noise: 0.000\n",
      "Iter 399/500 - Loss: 1.122   lengthscale: 31.171   noise: 0.000\n",
      "Iter 400/500 - Loss: 1.122   lengthscale: 31.253   noise: 0.000\n",
      "Iter 401/500 - Loss: 1.122   lengthscale: 31.334   noise: 0.000\n",
      "Iter 402/500 - Loss: 1.122   lengthscale: 31.416   noise: 0.000\n",
      "Iter 403/500 - Loss: 1.122   lengthscale: 31.497   noise: 0.000\n",
      "Iter 404/500 - Loss: 1.122   lengthscale: 31.578   noise: 0.000\n",
      "Iter 405/500 - Loss: 1.122   lengthscale: 31.660   noise: 0.000\n",
      "Iter 406/500 - Loss: 1.122   lengthscale: 31.741   noise: 0.000\n",
      "Iter 407/500 - Loss: 1.122   lengthscale: 31.822   noise: 0.000\n",
      "Iter 408/500 - Loss: 1.122   lengthscale: 31.903   noise: 0.000\n",
      "Iter 409/500 - Loss: 1.122   lengthscale: 31.983   noise: 0.000\n",
      "Iter 410/500 - Loss: 1.122   lengthscale: 32.064   noise: 0.000\n",
      "Iter 411/500 - Loss: 1.122   lengthscale: 32.145   noise: 0.000\n",
      "Iter 412/500 - Loss: 1.122   lengthscale: 32.225   noise: 0.000\n",
      "Iter 413/500 - Loss: 1.122   lengthscale: 32.306   noise: 0.000\n",
      "Iter 414/500 - Loss: 1.122   lengthscale: 32.386   noise: 0.000\n",
      "Iter 415/500 - Loss: 1.122   lengthscale: 32.467   noise: 0.000\n",
      "Iter 416/500 - Loss: 1.122   lengthscale: 32.547   noise: 0.000\n",
      "Iter 417/500 - Loss: 1.122   lengthscale: 32.627   noise: 0.000\n",
      "Iter 418/500 - Loss: 1.122   lengthscale: 32.707   noise: 0.000\n",
      "Iter 419/500 - Loss: 1.122   lengthscale: 32.787   noise: 0.000\n",
      "Iter 420/500 - Loss: 1.122   lengthscale: 32.867   noise: 0.000\n",
      "Iter 421/500 - Loss: 1.122   lengthscale: 32.947   noise: 0.000\n",
      "Iter 422/500 - Loss: 1.122   lengthscale: 33.027   noise: 0.000\n",
      "Iter 423/500 - Loss: 1.122   lengthscale: 33.106   noise: 0.000\n",
      "Iter 424/500 - Loss: 1.122   lengthscale: 33.186   noise: 0.000\n",
      "Iter 425/500 - Loss: 1.122   lengthscale: 33.266   noise: 0.000\n",
      "Iter 426/500 - Loss: 1.122   lengthscale: 33.345   noise: 0.000\n",
      "Iter 427/500 - Loss: 1.122   lengthscale: 33.424   noise: 0.000\n",
      "Iter 428/500 - Loss: 1.122   lengthscale: 33.504   noise: 0.000\n",
      "Iter 429/500 - Loss: 1.122   lengthscale: 33.583   noise: 0.000\n",
      "Iter 430/500 - Loss: 1.122   lengthscale: 33.662   noise: 0.000\n",
      "Iter 431/500 - Loss: 1.122   lengthscale: 33.741   noise: 0.000\n",
      "Iter 432/500 - Loss: 1.122   lengthscale: 33.820   noise: 0.000\n",
      "Iter 433/500 - Loss: 1.122   lengthscale: 33.899   noise: 0.000\n",
      "Iter 434/500 - Loss: 1.122   lengthscale: 33.978   noise: 0.000\n",
      "Iter 435/500 - Loss: 1.122   lengthscale: 34.057   noise: 0.000\n",
      "Iter 436/500 - Loss: 1.122   lengthscale: 34.136   noise: 0.000\n",
      "Iter 437/500 - Loss: 1.122   lengthscale: 34.214   noise: 0.000\n",
      "Iter 438/500 - Loss: 1.122   lengthscale: 34.293   noise: 0.000\n",
      "Iter 439/500 - Loss: 1.122   lengthscale: 34.372   noise: 0.000\n",
      "Iter 440/500 - Loss: 1.122   lengthscale: 34.450   noise: 0.000\n",
      "Iter 441/500 - Loss: 1.122   lengthscale: 34.528   noise: 0.000\n",
      "Iter 442/500 - Loss: 1.122   lengthscale: 34.607   noise: 0.000\n",
      "Iter 443/500 - Loss: 1.122   lengthscale: 34.685   noise: 0.000\n",
      "Iter 444/500 - Loss: 1.122   lengthscale: 34.763   noise: 0.000\n",
      "Iter 445/500 - Loss: 1.122   lengthscale: 34.841   noise: 0.000\n",
      "Iter 446/500 - Loss: 1.122   lengthscale: 34.919   noise: 0.000\n",
      "Iter 447/500 - Loss: 1.122   lengthscale: 34.997   noise: 0.000\n",
      "Iter 448/500 - Loss: 1.122   lengthscale: 35.075   noise: 0.000\n",
      "Iter 449/500 - Loss: 1.122   lengthscale: 35.153   noise: 0.000\n",
      "Iter 450/500 - Loss: 1.122   lengthscale: 35.231   noise: 0.000\n",
      "Iter 451/500 - Loss: 1.122   lengthscale: 35.309   noise: 0.000\n",
      "Iter 452/500 - Loss: 1.122   lengthscale: 35.386   noise: 0.000\n",
      "Iter 453/500 - Loss: 1.122   lengthscale: 35.464   noise: 0.000\n",
      "Iter 454/500 - Loss: 1.122   lengthscale: 35.541   noise: 0.000\n",
      "Iter 455/500 - Loss: 1.122   lengthscale: 35.619   noise: 0.000\n",
      "Iter 456/500 - Loss: 1.122   lengthscale: 35.696   noise: 0.000\n",
      "Iter 457/500 - Loss: 1.122   lengthscale: 35.774   noise: 0.000\n",
      "Iter 458/500 - Loss: 1.122   lengthscale: 35.851   noise: 0.000\n",
      "Iter 459/500 - Loss: 1.122   lengthscale: 35.928   noise: 0.000\n",
      "Iter 460/500 - Loss: 1.122   lengthscale: 36.005   noise: 0.000\n",
      "Iter 461/500 - Loss: 1.122   lengthscale: 36.082   noise: 0.000\n",
      "Iter 462/500 - Loss: 1.122   lengthscale: 36.159   noise: 0.000\n",
      "Iter 463/500 - Loss: 1.122   lengthscale: 36.236   noise: 0.000\n",
      "Iter 464/500 - Loss: 1.122   lengthscale: 36.313   noise: 0.000\n",
      "Iter 465/500 - Loss: 1.122   lengthscale: 36.390   noise: 0.000\n",
      "Iter 466/500 - Loss: 1.122   lengthscale: 36.467   noise: 0.000\n",
      "Iter 467/500 - Loss: 1.122   lengthscale: 36.544   noise: 0.000\n",
      "Iter 468/500 - Loss: 1.122   lengthscale: 36.620   noise: 0.000\n",
      "Iter 469/500 - Loss: 1.122   lengthscale: 36.697   noise: 0.000\n",
      "Iter 470/500 - Loss: 1.122   lengthscale: 36.774   noise: 0.000\n",
      "Iter 471/500 - Loss: 1.122   lengthscale: 36.850   noise: 0.000\n",
      "Iter 472/500 - Loss: 1.122   lengthscale: 36.927   noise: 0.000\n",
      "Iter 473/500 - Loss: 1.122   lengthscale: 37.003   noise: 0.000\n",
      "Iter 474/500 - Loss: 1.122   lengthscale: 37.079   noise: 0.000\n",
      "Iter 475/500 - Loss: 1.122   lengthscale: 37.156   noise: 0.000\n",
      "Iter 476/500 - Loss: 1.122   lengthscale: 37.232   noise: 0.000\n",
      "Iter 477/500 - Loss: 1.122   lengthscale: 37.308   noise: 0.000\n",
      "Iter 478/500 - Loss: 1.122   lengthscale: 37.384   noise: 0.000\n",
      "Iter 479/500 - Loss: 1.122   lengthscale: 37.460   noise: 0.000\n",
      "Iter 480/500 - Loss: 1.122   lengthscale: 37.536   noise: 0.000\n",
      "Iter 481/500 - Loss: 1.122   lengthscale: 37.612   noise: 0.000\n",
      "Iter 482/500 - Loss: 1.122   lengthscale: 37.688   noise: 0.000\n",
      "Iter 483/500 - Loss: 1.122   lengthscale: 37.764   noise: 0.000\n",
      "Iter 484/500 - Loss: 1.122   lengthscale: 37.840   noise: 0.000\n",
      "Iter 485/500 - Loss: 1.122   lengthscale: 37.916   noise: 0.000\n",
      "Iter 486/500 - Loss: 1.122   lengthscale: 37.991   noise: 0.000\n",
      "Iter 487/500 - Loss: 1.122   lengthscale: 38.067   noise: 0.000\n",
      "Iter 488/500 - Loss: 1.122   lengthscale: 38.142   noise: 0.000\n",
      "Iter 489/500 - Loss: 1.122   lengthscale: 38.218   noise: 0.000\n",
      "Iter 490/500 - Loss: 1.122   lengthscale: 38.294   noise: 0.000\n",
      "Iter 491/500 - Loss: 1.122   lengthscale: 38.369   noise: 0.000\n",
      "Iter 492/500 - Loss: 1.122   lengthscale: 38.444   noise: 0.000\n",
      "Iter 493/500 - Loss: 1.122   lengthscale: 38.520   noise: 0.000\n",
      "Iter 494/500 - Loss: 1.122   lengthscale: 38.595   noise: 0.000\n",
      "Iter 495/500 - Loss: 1.122   lengthscale: 38.670   noise: 0.000\n",
      "Iter 496/500 - Loss: 1.122   lengthscale: 38.745   noise: 0.000\n",
      "Iter 497/500 - Loss: 1.122   lengthscale: 38.820   noise: 0.000\n",
      "Iter 498/500 - Loss: 1.122   lengthscale: 38.896   noise: 0.000\n",
      "Iter 499/500 - Loss: 1.122   lengthscale: 38.971   noise: 0.000\n",
      "Iter 500/500 - Loss: 1.122   lengthscale: 39.046   noise: 0.000\n"
     ]
    }
   ],
   "source": [
    "training_iter = 500\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(inputsZipped)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (51) must match the size of tensor b (71) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     test_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m)\n\u001b[0;32m      9\u001b[0m     test_x_distributional \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((test_x, (\u001b[38;5;241m1e-3\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(test_x))), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     observed_pred \u001b[38;5;241m=\u001b[39m likelihood(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x_distributional\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Initialize plot\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     f, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\models\\exact_gp.py:330\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[0;32m    327\u001b[0m     (\n\u001b[0;32m    328\u001b[0m         predictive_mean,\n\u001b[0;32m    329\u001b[0m         predictive_covar,\n\u001b[1;32m--> 330\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[0;32m    332\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py:313\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[1;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# For efficiency - we can make things more efficient\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m joint_covar\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mmax_eager_kernel_size\u001b[38;5;241m.\u001b[39mvalue():\n\u001b[1;32m--> 313\u001b[0m     test_covar \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_covar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m test_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[0;32m    315\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m test_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:410\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.to_dense\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[1;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\kernels\\kernel.py:539\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    536\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    538\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 539\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     )\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\ch982\\.conda\\envs\\EDFA_env\\Lib\\site-packages\\gpytorch\\kernels\\scale_kernel.py:109\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[1;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m--> 109\u001b[0m     orig_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     outputscales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputscale\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_dim_is_batch:\n",
      "File \u001b[1;32md:\\ch982\\OneDrive - University of Cambridge\\Documents\\EDFA Modelling\\NoiseFigure\\Modelling\\UncertainInputsGP\\gpytorchKernelTesting\\UncertainKernel.py:77\u001b[0m, in \u001b[0;36mUncertainKernel.forward\u001b[1;34m(self, x1, x2, **params)\u001b[0m\n\u001b[0;32m     70\u001b[0m l2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03mNumerator Only\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m var_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlengthscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx1_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx2_var\u001b[49m\u001b[38;5;66;03m#self.covar_dist(x1_var, -x2_var, square_dist=False)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# print(var_dist.shape)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# temp = self.lengthscale + var_dist\u001b[39;00m\n\u001b[0;32m     83\u001b[0m x1_mean_ \u001b[38;5;241m=\u001b[39m x1_mean\u001b[38;5;241m.\u001b[39mdiv(var_dist)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (51) must match the size of tensor b (71) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 71)\n",
    "    test_x_distributional = torch.stack((test_x, (1e-3 * torch.ones_like(test_x))), dim=1)\n",
    "    observed_pred = likelihood(model(test_x_distributional))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.errorbar(train_x_mean.numpy(), train_y.numpy(), xerr=train_x_stdv, fmt='k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = torch.sin(test_x * (2 * math.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1264)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(test_y - observed_pred.mean))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDFA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
